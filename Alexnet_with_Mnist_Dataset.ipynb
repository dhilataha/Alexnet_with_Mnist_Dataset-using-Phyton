{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alexnet with Mnist Dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AFF-6ZRut33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as f\n",
        "\n",
        "\n",
        "def calc_loss_acc(labels, logits):\n",
        "    \"\"\"Function to compute loss value. Here, we used cross entropy \n",
        "    Args:\n",
        "        logits: 4D tensor. output tensor from segnet model, which is the output of softmax\n",
        "        labels: true labels tensor\n",
        "    Returns:\n",
        "        loss (cross_entropy_mean), accuracy, predicts(logits with softmax) \n",
        "    \"\"\"\n",
        "    # calc cross entropy mean  cross_entropy\n",
        "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='cross_entropy')\n",
        "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
        "    tf.summary.scalar(name='loss', tensor=cross_entropy_mean)\n",
        "\n",
        "\n",
        "    predicts = tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1))\n",
        "\n",
        "    accuracy = tf.reduce_mean(tf.cast(predicts, dtype=tf.float32))\n",
        "    tf.summary.scalar(name='accuracy', tensor=accuracy)\n",
        "\n",
        "    return cross_entropy_mean, accuracy, predicts\n",
        "\n",
        "\n",
        "def train_op(total_loss, global_steps, base_learning_rate, option='Adam'):\n",
        "    \"\"\"This function defines train optimizer \n",
        "    Args:\n",
        "        total_loss: the loss value\n",
        "        global_steps: global steps is used to track how many batch had been passed. In the training process, the initial value for global_steps = 0, here  \n",
        "        global_steps=tf.Variable(0, trainable=False). then after one batch of images passed, the loss is passed into the optimizer to update the weight, then the global \n",
        "        step increased by one.\n",
        "        base_learning_rate: default value 0.1\n",
        "    Returns:\n",
        "        the train optimizer\n",
        "    \"\"\"\n",
        "\n",
        "    # base_learning_rate = 0.01\n",
        "    # get update operation\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "\n",
        "        if option == 'Adam':\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=base_learning_rate)\n",
        "            print(\"Running with Adam Optimizer with learning rate:\", base_learning_rate)\n",
        "        elif option == 'SGD':\n",
        "            # base_learning_rate = 0.01\n",
        "            learning_rate_decay = tf.train.exponential_decay(base_learning_rate, global_steps, 1000, 0.0005)\n",
        "            optimizer = tf.train.GradientDescentOptimizer(learning_rate_decay)\n",
        "            print(\"Running with SGD Optimizer with learning rate:\", learning_rate_decay)\n",
        "        else:\n",
        "            raise ValueError('Optimizer is not recognized')\n",
        "\n",
        "        grads = optimizer.compute_gradients(total_loss, var_list=tf.trainable_variables())\n",
        "        training_op = optimizer.apply_gradients(grads, global_step=global_steps)\n",
        "\n",
        "    return training_op\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw8LQQgRvCH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def max_pooling(inputs, ksize, stride, padding, name):\n",
        "    \"\"\"Create max pooling layer \n",
        "    Args:\n",
        "        inputs: float32 4D tensor\n",
        "        ksize: a tuple of 2 int with (kernel_height, kernel_width)\n",
        "        stride: a tuple\n",
        "        padding: string. padding mode 'SAME', '\n",
        "        name: string\n",
        "        \n",
        "    Returns:\n",
        "        4D tensor of [batch_size, height, width, channels]\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        value = tf.nn.max_pool(inputs, ksize=[1, ksize[0], ksize[1], 1], strides=[1, stride[0], stride[1], 1], padding=padding, name=scope.name)\n",
        "    return value\n",
        "\n",
        "def dropout(inputs, keep_prob, name):\n",
        "    \"\"\"Dropout layer\n",
        "    Args:\n",
        "        inputs: float32 4D tensor \n",
        "        keep_prob: the probability of keep training sample\n",
        "        name: layer name to define\n",
        "    Returns:\n",
        "        4D tensor of [batch_size, height, width, channels]\n",
        "    \"\"\"\n",
        "    \n",
        "    return tf.nn.dropout(inputs, rate=(1-keep_prob), name=name)\n",
        "\n",
        "\n",
        "def norm(inputs, radius=4, name=None):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        value = tf.nn.lrn(inputs, depth_radius=radius, bias=1.0, alpha=1e-4, beta=0.75, name=name)\n",
        "    return value\n",
        "\n",
        "# def batch_norm(inputs, name):\n",
        "#     \"\"\"batch normalization layer\n",
        "\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "def conv2d(inputs, shape, padding, name):\n",
        "    \"\"\"Create convolution 2D layer\n",
        "    Args:\n",
        "        inputs: float32. 4D tensor\n",
        "        shape: the shape of kernel \n",
        "        padding: string. padding mode 'SAME',\n",
        "        name: corressponding layer's name\n",
        "    Returns:\n",
        "        Output 4D tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        # get weights value and record a summary protocol buffer with a histogram\n",
        "        weights = tf.get_variable('weights', shape=shape, initializer=tf.initializers.he_normal())\n",
        "        tf.summary.histogram(scope.name + 'weights', weights)\n",
        "\n",
        "        # get biases value and record a summary protocol buffer with a histogram\n",
        "        biases = tf.get_variable('biases', shape=shape[3], initializer=tf.initializers.random_normal())\n",
        "        tf.summary.histogram(scope.name + 'biases', biases)\n",
        "        # compute convlotion W * X + b, activiation function relu function\n",
        "        outputs = tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding=padding)\n",
        "        outputs = tf.nn.bias_add(outputs, biases)\n",
        "        outputs = tf.nn.relu(outputs, name=scope.name + 'relu')\n",
        "    return outputs\n",
        "\n",
        "def fc(inputs, shape, name):\n",
        "    \"\"\"Create fully collection layer \n",
        "    Args:\n",
        "        inputs: Float32. 2D tensor with shape [batch, input_units]\n",
        "        shape: Int. a tuple with [num_inputs, num_outputs]\n",
        "        name: sring. layer name\n",
        "    Returns:\n",
        "        Outputs fully collection tensor\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        weights = tf.get_variable('weights', shape = [shape[0], shape[1]], initializer=tf.initializers.he_normal())\n",
        "        biases = tf.get_variable('biases', shape = [shape[1]], initializer=tf.initializers.random_normal())\n",
        "        # outputs = tf.nn.xw_plus_b(inputs, weights, biases, name = scope.name)\n",
        "        outputs = tf.add(tf.matmul(inputs, weights), biases, name=scope.name)\n",
        "\n",
        "    return tf.nn.relu(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2W0oz7_vp2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def alexnet(inputs, num_classes, keep_prob):\n",
        "    \"\"\"Create alexnet model\n",
        "    \"\"\"\n",
        "    x = tf.reshape(inputs, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # first conv layer, downsampling layer, and normalization layer\n",
        "    conv1 = conv2d(x, shape=(11, 11, 1, 96), padding='SAME', name='conv1')\n",
        "    pool1 = max_pooling(conv1, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool1')\n",
        "    norm1 = norm(pool1, radius=4, name='norm1')\n",
        "\n",
        "    # second conv layer\n",
        "    conv2 = conv2d(norm1, shape=(5, 5, 96, 256), padding='SAME', name='conv2')\n",
        "    pool2 = max_pooling(conv2, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool2')\n",
        "    norm2 = norm(pool2, radius=4, name='norm2')\n",
        "\n",
        "    # 3rd conv layer\n",
        "    conv3 = conv2d(norm2, shape=(3, 3, 256, 384), padding='SAME', name='conv3')\n",
        "    # pool3 = max_pooling(conv3, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool3')\n",
        "    norm3 = norm(conv3, radius=4, name='norm3')\n",
        "\n",
        "    # 4th conv layer\n",
        "    conv4 = conv2d(norm3, shape=(3, 3, 384, 384), padding='SAME', name='conv4')\n",
        "\n",
        "    # 5th conv layer\n",
        "    conv5 = conv2d(conv4, shape=(3, 3, 384, 256), padding='SAME', name='conv5')\n",
        "    pool5 = max_pooling(conv5, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool5')\n",
        "    norm5 = norm(pool5, radius=4, name='norm5')\n",
        "\n",
        "    # first fully connected layer\n",
        "    fc1 = tf.reshape(norm5, shape=(-1, 4*4*256))\n",
        "    fc1 = fc(fc1, shape=(4*4*256, 4096), name='fc1')\n",
        "    fc1 = dropout(fc1, keep_prob=keep_prob, name='dropout1')\n",
        "\n",
        "    fc2 = fc(fc1, shape=(4096, 4096), name='fc2')\n",
        "    fc2 = dropout(fc2, keep_prob=keep_prob, name='dropout2')\n",
        "\n",
        "    # output logits value\n",
        "    with tf.variable_scope('classifier') as scope:\n",
        "        weights = tf.get_variable('weights', shape=[4096, num_classes], initializer=tf.initializers.he_normal())\n",
        "        biases = tf.get_variable('biases', shape=[num_classes], initializer=tf.initializers.random_normal())\n",
        "        # define output logits value\n",
        "        logits = tf.add(tf.matmul(fc2, weights), biases, name=scope.name + '_logits')\n",
        "\n",
        "    return logits\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQQGP9z1vwtC",
        "colab_type": "code",
        "outputId": "f6d368fe-0cb4-4788-bb4f-06d8f6d66e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "flags.DEFINE_integer('valid_steps', 11, 'The number of validation steps ')\n",
        "\n",
        "flags.DEFINE_integer('max_steps', 1001, 'The number of maximum steps for traing')\n",
        "\n",
        "flags.DEFINE_integer('batch_size', 128, 'The number of images in each batch during training')\n",
        "\n",
        "flags.DEFINE_float('base_learning_rate', 0.0001, \"base learning rate for optimizer\")\n",
        "\n",
        "flags.DEFINE_integer('input_shape', 784, 'The inputs tensor shape')\n",
        "\n",
        "flags.DEFINE_integer('num_classes', 10, 'The number of label classes')\n",
        "\n",
        "flags.DEFINE_string('save_dir', './outputs', 'The path to saved checkpoints')\n",
        "\n",
        "flags.DEFINE_float('keep_prob', 0.75, \"the probability of keeping neuron unit\")\n",
        "\n",
        "flags.DEFINE_string('tb_path', './tb_logs/', 'The path points to tensorboard logs ')\n",
        "\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "\n",
        "def train(FLAGS):\n",
        "    \"\"\"Training model\n",
        "    \"\"\"\n",
        "    valid_steps = FLAGS.valid_steps\n",
        "    max_steps = FLAGS.max_steps\n",
        "    batch_size = FLAGS.batch_size\n",
        "    base_learning_rate = FLAGS.base_learning_rate\n",
        "    input_shape = FLAGS.input_shape  # image shape = 28 * 28\n",
        "    num_classes = FLAGS.num_classes\n",
        "    keep_prob = FLAGS.keep_prob\n",
        "    save_dir = FLAGS.save_dir\n",
        "    tb_path = FLAGS.tb_path \n",
        "\n",
        "    train_loss, train_acc = [], []\n",
        "    valid_loss, valid_acc = [], []\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    # define default tensor graphe \n",
        "    with tf.Graph().as_default():\n",
        "        images_pl = tf.placeholder(tf.float32, shape=[None, input_shape])\n",
        "        labels_pl = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
        "\n",
        "        # define a variable global_steps\n",
        "        global_steps = tf.Variable(0, trainable=False)\n",
        "\n",
        "        # build a graph that calculate the logits prediction from model\n",
        "        logits = alexnet(images_pl, num_classes, keep_prob)\n",
        "\n",
        "        loss, acc, _ = calc_loss_acc(labels_pl, logits)\n",
        "\n",
        "        # build a graph that trains the model with one batch of example and updates the model params \n",
        "        training_op = train_op(loss, global_steps, base_learning_rate)\n",
        "\n",
        "        # define the model saver\n",
        "        saver = tf.train.Saver(tf.global_variables())\n",
        "        \n",
        "        # define a summary operation \n",
        "        summary_op = tf.summary.merge_all()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            # print(sess.run(tf.trainable_variables()))\n",
        "            # start queue runners\n",
        "            coord = tf.train.Coordinator()\n",
        "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "            train_writter = tf.summary.FileWriter(tb_path, sess.graph)\n",
        "\n",
        "            # start training\n",
        "            for step in range(100):\n",
        "                # get train image / label batch\n",
        "                train_image_batch, train_label_batch = mnist.train.next_batch(batch_size)\n",
        "\n",
        "                train_feed_dict = {images_pl: train_image_batch, labels_pl: train_label_batch}\n",
        "\n",
        "                _, _loss, _acc, _summary_op = sess.run([training_op, loss, acc, summary_op], feed_dict = train_feed_dict)\n",
        "\n",
        "                # store loss and accuracy value\n",
        "                train_loss.append(_loss)\n",
        "                train_acc.append(_acc)\n",
        "                print(\"Iteration \" + str(step) + \", Mini-batch Loss= \" + \"{:.6f}\".format(_loss) + \", Training Accuracy= \" + \"{:.5f}\".format(_acc))\n",
        "\n",
        "                if step % 100 == 0:\n",
        "                    _valid_loss, _valid_acc = [], []\n",
        "                    print('Start validation process')\n",
        "\n",
        "                    for step in range(valid_steps):\n",
        "                        valid_image_batch, valid_label_batch = mnist.test.next_batch(batch_size)\n",
        "\n",
        "                        valid_feed_dict = {images_pl: valid_image_batch, labels_pl: valid_label_batch}\n",
        "\n",
        "                        _loss, _acc = sess.run([loss, acc], feed_dict = valid_feed_dict)\n",
        "\n",
        "                        _valid_loss.append(_loss)\n",
        "                        _valid_acc.append(_acc)\n",
        "\n",
        "                    valid_loss.append(np.mean(_valid_loss))\n",
        "                    valid_acc.append(np.mean(_valid_acc))\n",
        "\n",
        "                    print(\"Iteration {}: Train Loss {:6.3f}, Train Acc {:6.3f}, Val Loss {:6.3f}, Val Acc {:6.3f}\".format(step, train_loss[-1], train_acc[-1], valid_loss[-1], valid_acc[-1]))\n",
        "            \n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'train_loss'), train_loss)\n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'train_acc'), train_acc)\n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'valid_loss'), valid_loss)\n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'valid_acc'), valid_acc)\n",
        "            checkpoint_path = os.path.join(save_dir, 'model', 'model.ckpt')\n",
        "            saver.save(sess, checkpoint_path, global_step=step)\n",
        "\n",
        "            coord.request_stop()\n",
        "            coord.join(threads)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    train(FLAGS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "Running with Adam Optimizer with learning rate: 0.0001\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "Iteration 0, Mini-batch Loss= 7.779362, Training Accuracy= 0.09375\n",
            "Start validation process\n",
            "Iteration 10: Train Loss  7.779, Train Acc  0.094, Val Loss  6.824, Val Acc  0.104\n",
            "Iteration 1, Mini-batch Loss= 8.402212, Training Accuracy= 0.04688\n",
            "Iteration 2, Mini-batch Loss= 9.462041, Training Accuracy= 0.13281\n",
            "Iteration 3, Mini-batch Loss= 9.505525, Training Accuracy= 0.11719\n",
            "Iteration 4, Mini-batch Loss= 8.242508, Training Accuracy= 0.10938\n",
            "Iteration 5, Mini-batch Loss= 5.228747, Training Accuracy= 0.14062\n",
            "Iteration 6, Mini-batch Loss= 5.201808, Training Accuracy= 0.08594\n",
            "Iteration 7, Mini-batch Loss= 4.166608, Training Accuracy= 0.12500\n",
            "Iteration 8, Mini-batch Loss= 3.516380, Training Accuracy= 0.15625\n",
            "Iteration 9, Mini-batch Loss= 3.624369, Training Accuracy= 0.08594\n",
            "Iteration 10, Mini-batch Loss= 3.195764, Training Accuracy= 0.12500\n",
            "Iteration 11, Mini-batch Loss= 3.035702, Training Accuracy= 0.10156\n",
            "Iteration 12, Mini-batch Loss= 2.876457, Training Accuracy= 0.14844\n",
            "Iteration 13, Mini-batch Loss= 3.002950, Training Accuracy= 0.10938\n",
            "Iteration 14, Mini-batch Loss= 2.629632, Training Accuracy= 0.14062\n",
            "Iteration 15, Mini-batch Loss= 2.362460, Training Accuracy= 0.21094\n",
            "Iteration 16, Mini-batch Loss= 2.751668, Training Accuracy= 0.19531\n",
            "Iteration 17, Mini-batch Loss= 2.726039, Training Accuracy= 0.22656\n",
            "Iteration 18, Mini-batch Loss= 2.376962, Training Accuracy= 0.23438\n",
            "Iteration 19, Mini-batch Loss= 2.381524, Training Accuracy= 0.21875\n",
            "Iteration 20, Mini-batch Loss= 2.350482, Training Accuracy= 0.22656\n",
            "Iteration 21, Mini-batch Loss= 2.269847, Training Accuracy= 0.21875\n",
            "Iteration 22, Mini-batch Loss= 2.102950, Training Accuracy= 0.26562\n",
            "Iteration 23, Mini-batch Loss= 2.048562, Training Accuracy= 0.28125\n",
            "Iteration 24, Mini-batch Loss= 1.983227, Training Accuracy= 0.27344\n",
            "Iteration 25, Mini-batch Loss= 2.066859, Training Accuracy= 0.35156\n",
            "Iteration 26, Mini-batch Loss= 2.090015, Training Accuracy= 0.26562\n",
            "Iteration 27, Mini-batch Loss= 1.815060, Training Accuracy= 0.40625\n",
            "Iteration 28, Mini-batch Loss= 1.944674, Training Accuracy= 0.35938\n",
            "Iteration 29, Mini-batch Loss= 1.823680, Training Accuracy= 0.32812\n",
            "Iteration 30, Mini-batch Loss= 1.687352, Training Accuracy= 0.41406\n",
            "Iteration 31, Mini-batch Loss= 1.664567, Training Accuracy= 0.48438\n",
            "Iteration 32, Mini-batch Loss= 1.768838, Training Accuracy= 0.40625\n",
            "Iteration 33, Mini-batch Loss= 1.519465, Training Accuracy= 0.43750\n",
            "Iteration 34, Mini-batch Loss= 1.366076, Training Accuracy= 0.54688\n",
            "Iteration 35, Mini-batch Loss= 1.378095, Training Accuracy= 0.44531\n",
            "Iteration 36, Mini-batch Loss= 1.456127, Training Accuracy= 0.46875\n",
            "Iteration 37, Mini-batch Loss= 1.335669, Training Accuracy= 0.55469\n",
            "Iteration 38, Mini-batch Loss= 1.356871, Training Accuracy= 0.53906\n",
            "Iteration 39, Mini-batch Loss= 1.388165, Training Accuracy= 0.51562\n",
            "Iteration 40, Mini-batch Loss= 1.203805, Training Accuracy= 0.57812\n",
            "Iteration 41, Mini-batch Loss= 1.040689, Training Accuracy= 0.65625\n",
            "Iteration 42, Mini-batch Loss= 1.207106, Training Accuracy= 0.60938\n",
            "Iteration 43, Mini-batch Loss= 1.125682, Training Accuracy= 0.66406\n",
            "Iteration 44, Mini-batch Loss= 0.999239, Training Accuracy= 0.66406\n",
            "Iteration 45, Mini-batch Loss= 0.863969, Training Accuracy= 0.72656\n",
            "Iteration 46, Mini-batch Loss= 1.149705, Training Accuracy= 0.64062\n",
            "Iteration 47, Mini-batch Loss= 0.820900, Training Accuracy= 0.75781\n",
            "Iteration 48, Mini-batch Loss= 0.941668, Training Accuracy= 0.67969\n",
            "Iteration 49, Mini-batch Loss= 0.726044, Training Accuracy= 0.78125\n",
            "Iteration 50, Mini-batch Loss= 0.974881, Training Accuracy= 0.67188\n",
            "Iteration 51, Mini-batch Loss= 0.985037, Training Accuracy= 0.64844\n",
            "Iteration 52, Mini-batch Loss= 0.838246, Training Accuracy= 0.67969\n",
            "Iteration 53, Mini-batch Loss= 0.745131, Training Accuracy= 0.75000\n",
            "Iteration 54, Mini-batch Loss= 0.636669, Training Accuracy= 0.79688\n",
            "Iteration 55, Mini-batch Loss= 0.624745, Training Accuracy= 0.78125\n",
            "Iteration 56, Mini-batch Loss= 0.728487, Training Accuracy= 0.73438\n",
            "Iteration 57, Mini-batch Loss= 0.697720, Training Accuracy= 0.75000\n",
            "Iteration 58, Mini-batch Loss= 0.626647, Training Accuracy= 0.75000\n",
            "Iteration 59, Mini-batch Loss= 0.669904, Training Accuracy= 0.76562\n",
            "Iteration 60, Mini-batch Loss= 0.707533, Training Accuracy= 0.70312\n",
            "Iteration 61, Mini-batch Loss= 0.707320, Training Accuracy= 0.77344\n",
            "Iteration 62, Mini-batch Loss= 0.975862, Training Accuracy= 0.74219\n",
            "Iteration 63, Mini-batch Loss= 0.683570, Training Accuracy= 0.78125\n",
            "Iteration 64, Mini-batch Loss= 0.538965, Training Accuracy= 0.79688\n",
            "Iteration 65, Mini-batch Loss= 0.667489, Training Accuracy= 0.79688\n",
            "Iteration 66, Mini-batch Loss= 0.609832, Training Accuracy= 0.79688\n",
            "Iteration 67, Mini-batch Loss= 0.664096, Training Accuracy= 0.81250\n",
            "Iteration 68, Mini-batch Loss= 0.567016, Training Accuracy= 0.79688\n",
            "Iteration 69, Mini-batch Loss= 0.599069, Training Accuracy= 0.78125\n",
            "Iteration 70, Mini-batch Loss= 0.523788, Training Accuracy= 0.82031\n",
            "Iteration 71, Mini-batch Loss= 0.461914, Training Accuracy= 0.86719\n",
            "Iteration 72, Mini-batch Loss= 0.597401, Training Accuracy= 0.82031\n",
            "Iteration 73, Mini-batch Loss= 0.528867, Training Accuracy= 0.80469\n",
            "Iteration 74, Mini-batch Loss= 0.477547, Training Accuracy= 0.84375\n",
            "Iteration 75, Mini-batch Loss= 0.579558, Training Accuracy= 0.83594\n",
            "Iteration 76, Mini-batch Loss= 0.357325, Training Accuracy= 0.88281\n",
            "Iteration 77, Mini-batch Loss= 0.430109, Training Accuracy= 0.85938\n",
            "Iteration 78, Mini-batch Loss= 0.545567, Training Accuracy= 0.84375\n",
            "Iteration 79, Mini-batch Loss= 0.543062, Training Accuracy= 0.81250\n",
            "Iteration 80, Mini-batch Loss= 0.444077, Training Accuracy= 0.91406\n",
            "Iteration 81, Mini-batch Loss= 0.531604, Training Accuracy= 0.82812\n",
            "Iteration 82, Mini-batch Loss= 0.519700, Training Accuracy= 0.80469\n",
            "Iteration 83, Mini-batch Loss= 0.494740, Training Accuracy= 0.88281\n",
            "Iteration 84, Mini-batch Loss= 0.441064, Training Accuracy= 0.90625\n",
            "Iteration 85, Mini-batch Loss= 0.402180, Training Accuracy= 0.84375\n",
            "Iteration 86, Mini-batch Loss= 0.345111, Training Accuracy= 0.89062\n",
            "Iteration 87, Mini-batch Loss= 0.395185, Training Accuracy= 0.88281\n",
            "Iteration 88, Mini-batch Loss= 0.610902, Training Accuracy= 0.81250\n",
            "Iteration 89, Mini-batch Loss= 0.386108, Training Accuracy= 0.88281\n",
            "Iteration 90, Mini-batch Loss= 0.497455, Training Accuracy= 0.85156\n",
            "Iteration 91, Mini-batch Loss= 0.403090, Training Accuracy= 0.86719\n",
            "Iteration 92, Mini-batch Loss= 0.370018, Training Accuracy= 0.89062\n",
            "Iteration 93, Mini-batch Loss= 0.290767, Training Accuracy= 0.90625\n",
            "Iteration 94, Mini-batch Loss= 0.334459, Training Accuracy= 0.88281\n",
            "Iteration 95, Mini-batch Loss= 0.232858, Training Accuracy= 0.89844\n",
            "Iteration 96, Mini-batch Loss= 0.486995, Training Accuracy= 0.85156\n",
            "Iteration 97, Mini-batch Loss= 0.297684, Training Accuracy= 0.87500\n",
            "Iteration 98, Mini-batch Loss= 0.290028, Training Accuracy= 0.91406\n",
            "Iteration 99, Mini-batch Loss= 0.388245, Training Accuracy= 0.87500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-62caed097b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-62caed097b82>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(FLAGS)\u001b[0m\n\u001b[1;32m    122\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration {}: Train Loss {:6.3f}, Train Acc {:6.3f}, Val Loss {:6.3f}, Val Acc {:6.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './outputs/accuracy_loss/train_loss.npy'"
          ]
        }
      ]
    }
  ]
}